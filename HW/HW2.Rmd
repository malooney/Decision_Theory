---
title: "STAT 5370 – Decision Theory"
subtitle: "Homework 2"
author: "Matthew Aaron Looney"
date: "6/22/2018"
output: 
  pdf_document: 
    fig_caption: yes
    toc: yes
    toc_depth: 4
header-includes:
- \usepackage{graphicx}
- \usepackage{rotating}
- \usepackage{longtable}
- \usepackage{amssymb,amsmath}
- \usepackage{dcolumn}
- \usepackage{setspace}
- \usepackage{mathrsfs}
- \usepackage{eso-pic,graphicx,transparent}

bibliography: DTheory_HW2Bib.bib
csl: computational-economics.csl
link-citations: true
fontsize: 10pt
geometry: margin = 1.25in
---

```{r, echo=F, warning=F, message=F, cache=F}

# Housekeeping ----------------------------------------------------------------
rm(list= ls())
cat("\014")

set.seed(1)


```


\newpage

<!-- \onehalfspacing -->

# Problem 1

In this problem we will explore an alternate class of priors.

### (a) Find a resource and write a basic description of what a Jeffrey’s prior is. Be thorough, your exposition here needs to be such that someone who knows nothing of what a Jeffrey’s prior is could learn the salient features. Identify why people like Jeffery’s priors and discuss drawbacks.

The general idea behind the Jeffrys prior is to find a prior that is non-informative. In other words, we as the researcher should not impose too much prior belief onto the model. We want the data to be allowed to speak for itself. Further, we may be in a situation where there is not much information available about the population under study and we need a way to develop a prior despite this lack of initial knowledge.

Sir Harold Jeffreys developed a method to obtain a non-informative prior based on a one-to-one transformation of the distributions parameters. This is sometimes refereed to as the Jeffreys’ invariance principle because the parameters are invariant to affine transformations.

The Jeffreys prior is derived by the following steps:

  * Based on the proposed DGP, identify a distribution for the data.
  
  * Derive a likelihood function based on the data distribution
  
  * Ensure the likelihood function is twice differentiable
  
  * Calculate the Fisher Information Matrix
    * alternative forms of the Fisher information matrix may be employed depending on the nature of the independence of the data, ie. independent or iid. If the distribution is single parameter the calculation is vastly simplified.
  
  * The Jeffreys prior is given by the following formula: $\pi(\theta) \propto \sqrt{det|\bf{I{(\theta)}}|}$ 
    * where $\bf{I}(\theta)$ is the Fisher information matrix, and $det|\cdot|$ is the determinant if the information matrix.
    
One of the benefits of using the Jeffreys prior is that it brings very little bias into the model. Using the Jeffreys prior reduces some of the criticism that the researcher is driving the results based on the prior distribution. As mentioned earlier, there may be little information available about the population model under study and having access to a non-informative prior can help. The Jeffreys prior can sometimes be very difficult or impossible to calculate depending on the number of parameters in the model and the nature of the independence of the data. In high dimensional space the Jeffreys prior can be challenging to obtain, and if obtainable can sometimes yield poor results.
  


### (b) Derive the Jeffery’s prior for the Bernoulli model (i.e., assuming ${Y_i}\mathop  = \limits^{iid} Bernoulli(p)$), and find the posterior distribution of $p$ under this prior. Is it a known distribution? Discuss how you would proceed to conduct posterior inference.

Assuming ${Y_i}\mathop  = \limits^{iid} Bernoulli(p)$, then we can say that $Y|p \sim Binomial(n,p)$ and we can obtain the likelihood function as:

$$L(p|Y) = \prod\limits_{i = 1}^n {{p^{{y_i}}}{{(1 - p)}^{1 - {y_i}}} = {p^{\sum\nolimits_{i = 1}^n {{y_i}} }}{{(1 - p)}^{\sum\nolimits_{i = 1}^n {(1 - {y_i})} }}}  = {p^{\sum\nolimits_{i = 1}^n {{y_i}} }}{(1 - p)^{n - \sum\nolimits_{i = 1}^n {{y_i}} }}$$
Letting $k={\sum\nolimits_{i = 1}^n {{y_i}} }$ we obtain the log likelihood as:

$$\log L(p|Y) = k\log (p) + (n - k)\log (1 - p)$$

The next step in the derivation of the Jeffreys Prior is to calculate the Fisher information matrix. We are told the data are iid and we can see that the log likelihood function is twice differentiable which allows us to use an alternative form of the Fisher information matrix. Further, calculation is vastly simplified since our likelihood function is low dimensional (single parameter). The Fisher information is given by the following:

\begin{equation}
\tag{General}
{\bf{I}_{j,k}}(\boldsymbol{\theta}) =  - \mathbb{E}_{\bf{y}|\theta}\left[ {{{{\partial ^2}\log L(\bf{y}|\boldsymbol{\theta})} \over {\partial {\theta _j}\partial {\theta _k}}}} \right]
\end{equation}

\begin{equation}
{\bf{I}}(p) =  - \left[ {{{ -\mathbb{E} [k]} \over {{p^2}}} -{{(n -\mathbb{E}[k])} \over {{{(1 - p)}^2}}}} \right]
\end{equation}

Recalling that $Y|p \sim Binomial(n,p)$ and we defined $k$ as ${\sum\nolimits_{i = 1}^n {{y_i}} }$, then $\mathbb{E}[k]=np$ and we obtain the following results:

$${\bf{I}}(p) = {n \over {p(1 - p)}}$$

The Jefferys prior is defined as:

\begin{equation}
\tag{General}
\pi(\theta) \propto \sqrt{det|\bf{I{(\theta)}}|}
\end{equation}

\begin{equation}
\pi(p) \propto \sqrt{det|\bf{I{(p)}}|} \propto {p^{ - 0.5}}{(1 - p)^{ - 0.5}}
\end{equation}

We recognize that equation 2 can be made to look like a Beta distribution by subtracting one from each of the powers to obtain $\pi(p) \propto {p^{0.5-1}}{(1 - p)^{0.5-1}}$ and we now have the Jeffrys prior which is $Beta(\frac{1}{2}, \frac{1}{2})$.

With the non-informative Jefferys prior in hand we can proceed to build the posterior distribution in the usual way which gives:

\begin{equation}
\tag{General}
p(\theta |y) = \frac{{p(y|\theta )p(\theta )}}{{\int\limits_A {p(y|\theta )p(\theta )d\theta } }} \propto \underbrace {p(y|\theta )}_{{\text{L(}}\theta {\text{)}}}\underbrace {p(\theta )}_{{\text{Prior}}}
\end{equation}

\begin{equation}
\begin{split}
\pi (p|y) &= {p^{\sum\nolimits_{i = 1}^n {{y_i}} }}{(1 - p)^{n - \sum\nolimits_{i = 1}^n {{y_i}} }} \cdot {p^{0.5 - 1}}{(1 - p)^{0.5 - 1}} \\
& = {p^{\sum\nolimits_{i = 1}^n {{y_i} + 0.5 - 1} }}{(1 - p)^{n - \sum\nolimits_{i = 1}^n {{y_i} + 0.5 - 1} }}
\end{split}
\end{equation}

If we let ${\sum\nolimits_{i = 1}^n {{y_i}} }= n \bar{Y}$ then the posterior distribution is $Beta(\overbrace {n\bar Y + 0.5}^\alpha,\overbrace {n - n\bar Y + 0.5}^\beta)$, where $\alpha$ and $\beta$ are the parameters of the Beta distribution and $\alpha>0$ (first shape parameter), $\beta>0$ (second shape parameter).

The Beta distribution is well known with all of the most relevant moments documented. We can obtain the posterior mean and variance by applying the following formulas and making the appropriate substitutions of the posterior distributions parameters:

\begin{equation}
\begin{split}
\mathbb{E}[p|y] &= \frac{\alpha}{\alpha+\beta}= \hat\mu \\
Var(p|y) &= \frac{\hat\mu(1-\hat\mu)}{\alpha+\beta+1}
\end{split}
\end{equation}

We can also calculate the Equal Tail Credible Intervals and the HPD Credible Intervals to determine the 95% probability that $p$ falls within some upper and lower bound.



### (c) Derive the Jeffery’s prior for the Poisson model (i.e., assuming ${Y_i}\mathop  = \limits^{iid} Poisson(\lambda)$), and find the posterior distribution of $\lambda$ under this prior. Is it a known distribution? Discuss how you would proceed to conduct posterior inference.

Assuming ${Y_i}\mathop  = \limits^{iid} Poisson(\lambda)$, then we can say that $Y|\lambda \sim Poisson(\lambda)$ and we can obtain the likelihood function as:

$$
L(\lambda |y) = \prod\limits_{i = 1}^n {{{{e^{ - \lambda }}{\lambda ^y}} \over {y!}}}  = {{{e^{ - n\lambda }}{\lambda ^{\sum\limits_{i = 1}^n {{y_i}} }}} \over {\prod\limits_{i = 1}^n {y!} }} \propto {e^{ - n\lambda }}{\lambda ^{\sum\limits_{i = 1}^n {{y_i}} }}
$$
Letting $k={\sum\nolimits_{i = 1}^n {{y_i}} }$ we obtain the log likelihood as:

$$\log L(\lambda |Y) =  - n\lambda  + k\log (\lambda )$$

The next step in the derivation of the Jeffreys Prior is to calculate the Fisher information matrix. We are told the data are iid and we can see that the log likelihood function is twice differentiable which allows us to use an alternative form of the Fisher information matrix. Further, calculation is vastly simplified since our likelihood function is low dimensional (single parameter). The Fisher information is given by the following:

\begin{equation}
\tag{General}
{\bf{I}_{j,k}}(\boldsymbol{\theta}) =  - \mathbb{E}_{\bf{y}|\theta}\left[ {{{{\partial ^2}\log L(\bf{y}|\boldsymbol{\theta})} \over {\partial {\theta _j}\partial {\theta _k}}}} \right]
\end{equation}

\begin{equation}
\bf{I}(\lambda) = - \frac{\mathbb{E}[k]}{\lambda^2}
\end{equation}

Recalling that $Y|\lambda \sim Poisson(\lambda)$ and we defined $k$ as ${\sum\nolimits_{i = 1}^n {{y_i}} }=n\bar{Y}$, then $n\mathbb{E}[\bar{Y}]=n\lambda$ and we obtain the following results:

$$\bf{I}(\lambda)=\frac{n}{\lambda}$$
The Jefferys prior is defined as:

\begin{equation}
\tag{General}
\pi(\theta) \propto \sqrt{det|\bf{I{(\theta)}}|}
\end{equation}

\begin{equation}
\pi(\lambda) \propto \sqrt{det|\bf{I{(\lambda)}}|} \propto \lambda^{-0.5}
\end{equation}

We recognize that equation 6 can be made to look like a Gamma distribution by subtracting one from the power on $\lambda$ to obtain $\pi(\lambda) \propto \lambda^{0.5-1}e^0$ and we now have the Jeffrys prior which is $Gamma(\frac{1}{2}, 0)$.

With the non-informative Jefferys prior in hand we can proceed to build the posterior distribution in the usual way which gives:

\begin{equation}
\tag{General}
p(\theta |y) = \frac{{p(y|\theta )p(\theta )}}{{\int\limits_A {p(y|\theta )p(\theta )d\theta } }} \propto \underbrace {p(y|\theta )}_{{\text{L(}}\theta {\text{)}}}\underbrace {p(\theta )}_{{\text{Prior}}}
\end{equation}

\begin{equation}
\begin{split}
\pi (\lambda|y) &= {e^{ - n\lambda }}{\lambda ^{\sum\nolimits_{i = 1}^n {{y_i}} }} \cdot {e^0}{\lambda ^{0.5 - 1}} \\
& = {\lambda ^{\sum\nolimits_{i = 1}^n {{y_i}}  + 0.5 - 1}}{e^{ - n\lambda }}
\end{split}
\end{equation}

If we let ${\sum\nolimits_{i = 1}^n {{y_i}} }= n \bar{Y}$ then the posterior distribution is $Gamma(\overbrace {n\bar Y + 0.5}^\alpha,\overbrace {n}^\beta)$, where $\alpha$ and $\beta$ are the parameters of the Gamma distribution and $\alpha>0$ (shape parameter), $\beta>0$ (rate parameter).

The Gamma distribution is well known with all of the most relevant moments documented. We can obtain the posterior mean and variance by applying the following formulas and making the appropriate substitutions of the posterior distributions parameters:

\begin{equation}
\begin{split}
\mathbb{E}[\lambda|y] &= {\alpha  \over \beta } \\
Var(\lambda |y) &= {\alpha  \over \beta ^2}
\end{split}
\end{equation}

We can also calculate the Equal Tail Credible Intervals and the HPD Credible Intervals to determine the 95% probability that $\lambda$ falls within some upper and lower bound.


### (d) Derive the Jeffery’s prior for the exponential model (i.e., assuming ${Y_i}\mathop  = \limits^{iid} Exponential(\beta)$), and find the posterior distribution of $\beta$ under this prior. Is it a known distribution? Discuss how you would proceed to conduct posterior inference.

Assuming ${Y_i}\mathop  = \limits^{iid} Exponential(\beta)$, then we can say that $Y|\beta \sim Exponential(\beta)$ and we can obtain the likelihood function as:

$$L(\beta |y) = \prod\limits_{i = 1}^n {\beta {e^{ - \beta {y_i}}}}  = {\beta ^n}{e^{ - \beta \sum\nolimits_{i = 1}^n {{y_i}} }}$$
Letting $k={\sum\nolimits_{i = 1}^n {{y_i}} }$ we obtain the log likelihood as:

$$\log L(\beta |Y) = n\log (\beta ) - \beta k$$
The next step in the derivation of the Jeffreys Prior is to calculate the Fisher information matrix. We are told the data are iid and we can see that the log likelihood function is twice differentiable which allows us to use an alternative form of the Fisher information matrix. Further, calculation is vastly simplified since our likelihood function is low dimensional (single parameter). The Fisher information is given by the following:

\begin{equation}
\tag{General}
{\bf{I}_{j,k}}(\boldsymbol{\theta}) =  - \mathbb{E}_{\bf{y}|\theta}\left[ {{{{\partial ^2}\log L(\bf{y}|\boldsymbol{\theta})} \over {\partial {\theta _j}\partial {\theta _k}}}} \right]
\end{equation}

\begin{equation}
{\bf{I}}(\beta ) = {n \over {{\beta ^2}}}
\end{equation}

The Jefferys prior is defined as:

\begin{equation}
\tag{General}
\pi(\theta) \propto \sqrt{det|\bf{I{(\theta)}}|}
\end{equation}

\begin{equation}
\pi(\beta) \propto \sqrt{det|\bf{I{(\beta)}}|} \propto \beta^{-1}
\end{equation}

We recognize that equation 10 can be made to look like a Gamma distribution by the following $\beta^{0-1}e^0$ and we now have the Jeffrys prior which is $Gamma(0, 0)$. @Kerman:2011un has argued that a more proper way to represent improper priors is by representing this form of Jeffreys prior as $Gamma(\varepsilon, \varepsilon)$ where $\varepsilon \approx 0$. The rationale being that as $\varepsilon \to 0$, the distribution appears flatter and flatter over the positive real axis. Either way the prior is still improper but is now defined. The posterior distribution is, however, proper and we can proceed.

With the non-informative Jefferys prior in hand we can move to build the posterior distribution in the usual way which gives:

\begin{equation}
\tag{General}
p(\theta |y) = \frac{{p(y|\theta )p(\theta )}}{{\int\limits_A {p(y|\theta )p(\theta )d\theta } }} \propto \underbrace {p(y|\theta )}_{{\text{L(}}\theta {\text{)}}}\underbrace {p(\theta )}_{{\text{Prior}}}
\end{equation}

\begin{equation}
\begin{split}
p(\beta |y) &= {\beta ^n}{e^{ - \beta \sum\nolimits_{i = 1}^n {{y_i}} }} \cdot \beta^{0-1}e^0 \\
&= {\beta ^{n - 1}}{e^{ - \beta \sum\nolimits_{i = 1}^n {{y_i}} }}
\end{split}
\end{equation}

If we let ${\sum\nolimits_{i = 1}^n {{y_i}} }= n \bar{Y}$ then the posterior distribution is $Gamma(\overbrace {n}^\alpha,\overbrace {n \bar{Y}}^\beta)$, where $\alpha$ and $\beta$ are the parameters of the Gamma distribution and $\alpha>0$ (shape parameter), $\beta>0$ (rate parameter).

The Gamma distribution is well known with all of the most relevant moments documented. We can obtain the posterior mean and variance by applying the following formulas and making the appropriate substitutions of the posterior distributions parameters:

\begin{equation}
\begin{split}
\mathbb{E}[\beta|y] &= {\alpha  \over \beta } \\
Var(\beta |y) &= {\alpha  \over \beta ^2}
\end{split}
\end{equation}

We can also calculate the Equal Tail Credible Intervals and the HPD Credible Intervals to determine the 95% probability that $\lambda$ falls within some upper and lower bound.


# Problem 2

Let ${X_1} \sim N({\mu _1},\sigma _1^2)$ and ${X_2} \sim N({\mu _2},\sigma _2^2)$ such that $X_1$ and $X_2$ are independent. Define $Y = {X_1} + {X_2}$. The goal of this problem is to determine the distribution of $Y$; i.e., the sum of two independent normal random variables.

### (a) Obviously, this is trivial, so simply state the distribution of $Y$.

The sum of two independent normally distributed random variables is normal.

$$Y=X_1+X_2=Y \sim \mathcal{N}(\mu_1+\mu_2,\,\sigma_1^{2}+\sigma_2^{2})\,.$$

### (b) Approach 1: Consider using Monte Carlo sampling to obtain a histogram and kernel density estimate (see the code that I have provided) of the pdf of $Y$ by directly sampling both $X_1$ and $X_2$. Over plot the true density of $Y$ and comment. Note, you should make use of large enough Monte Carlo sample that your results are reasonable.

```{r, echo=F}

# Problem 2.b -----------------------------------------------------------------

mu1= runif(1, -5, 5)
mu2= runif(1, -5, 5)
sig2.1=runif(1, 0, 10)
sig2.2= runif(1, 0, 10)

n1= n2= 10000
X1= rnorm(n1, mu1, sqrt(sig2.1))
X2= rnorm(n2, mu2, sqrt(sig2.2))
Y= X1+ X2

mu= mu1+ mu2
sig.sq= sig2.1+ sig2.2

Y.grid= seq(-20, 20, by= 0.01)
true.density= 1/ sqrt(2* pi* sig.sq)* exp(-1/ (2* sig.sq)* (Y.grid- mu)^2)

Y.mean <- mean(Y)
Y.var <- var(Y)

```

```{r, echo=F, fig.height=3}

# Problem 2.b Plots------------------------------------------------------------

par(mfrow= c(1, 2), ps= 10, cex= 0.55, cex.main= 1.5)

hist(Y, prob= TRUE, xlab= "Y", ylab= "p(Y)", xlim= c(-30, 30))

lines(density(Y), lwd= 2, col= "green")

lines(Y.grid, true.density, col= "red", lty= 2, lwd= 2)

legend(4, 0.1,  c("True Density of Y", "MC Simulation of Y"), 
       lty= c(2, 1), col= c("red", "black"))


hist(Y, prob= TRUE, xlab= "Y", ylab= "p(Y)", xlim= c(-10, 5), 
     ylim= c(0.06, 0.11), main= "Zoomed Histogram of Y")

lines(density(Y), lwd= 2, col= "green")

lines(Y.grid, true.density, col= "red", lty= 1, lwd= 1)

legend(-3, 0.15,  c("True Density of Y", "MC Simulation of Y"), 
       lty= c(2, 1), col= c("red", "green"))

```

The above plots show the true density of $Y$ (red) and the Monte Carlo simulation results (black). The histogram plot shows the probability distribution of $Y$. It is plain to see that the true density of $Y$ and the Monte Carlo simulation results produce nearly identical results. The table below summarizes the results from this experiment. The last column of the Table 1 shows the difference between the true values of $Y$ and the Monte Carlo simulation. This indicates that the Monte Carlo simulation technique works very well in this case.  

\bigskip

\begin{table}
\begin{tabular}{ c c c c c }
& Initial Values & True Values & MC Simulation (10,000 iter) & $\Delta_{(\mu, \sigma^2)}$ \\ \hline \hline \\
$\mu_1$ & `r mu1 ` & . & . & . \\
$\mu_2$ & `r mu2 ` & . & . & . \\
$\sigma_1^2$ & `r sig2.1 ` & . & . & . \\
$\sigma_1^2$ & `r sig2.2 ` & . & . & . \\
${\mu}=\mu_1+\mu_2$ & . & `r mu ` & . & . \\
${\sigma^2}=\sigma^2_1+\sigma^2_2$ & . & `r sig.sq ` & . & . \\
$mc.\mu$ & . & . & `r Y.mean ` & . \\
$mc.\sigma^2$ & . & . & `r Y.var ` & . \\
$\Delta_{\mu}=\mu-mc.\mu$ & . & . & . & `r mu- Y.mean ` \\
$\Delta_{\sigma^2}=\sigma^2-mc.\sigma^2$ & . & . & . & `r sig.sq- Y.var ` \\ \hline
\end{tabular}
\caption{Summary of Results}
\label{table:kysymys}
\end{table}

### (c) Approach 2: Note, the distribution of $Y$ can also be obtained through the convolution of the probability distributions of $X_1$ and $X_2$. Sketch out theoretically how this would be done. Based on this idea, create a Monte Carlo sampling technique which can be used to approximate the pdf of $Y$ evaluated at any point in the support. Use this function and add the approximation based on this technique to the Figure described in the part (b) above.

```{r, echo=F}

# Problem 2.c -----------------------------------------------------------------
# Initial paramater values taken from Problem 1.b

# MC convolution --------------------------------------------------------------
N= length(Y.grid)
dY= numeric(N)
for(i in seq_len(N)){ 
  X2.x2 = rnorm(N, mu2, sqrt(sig2.2))
  dY[i] = (1/ N) *sum((dnorm(Y.grid[i]- X2.x2, mu1, sqrt(sig2.1))))
}

# Numerical Convolution using R "integrate" funtion (quadrature) --------------
f.X1 <- function(x1){ 
  dnorm(x1, mu1, sqrt(sig2.1))
}

f.X2 <- function(x2){
  dnorm(x2, mu2, sqrt(sig2.2))
}

f.Y <- function(Y.grid){ 
  integrate(function(x1, Y.grid) 
    f.X2(Y.grid- x1)* f.X1(x1), -Inf, Inf, Y.grid)$value 
}

f.Y <- Vectorize(f.Y)

# Y.grid is assumed to be the MC integration support region  
convolution.Y <- f.Y(Y.grid)

```

I have leaned very heavily on the several resources to learn the concept of convolutions as it relates to the sums of two continuous random variables. (@Grinstead:1997ww, @Robert:2010vu, @casella2002statistical) 

Let $X_1$ and $X_2$ be two continuous random variables with density functions $f(x)$ and $g(x)$, respectively. Assume that both $f(x_1)$ and $g(x_2)$ are defined for all real numbers, $x_1$ and $x_2$ $\in \mathbb{R}$. Then the convolution $f*g$ of $f$ and $g$ is the function given by

\begin{equation}
\begin{split}
{f_Y}(y) = (f*g)(y) &= \int_{ - \infty }^\infty  {f(y - {x_2})} g({x_2})d{x_2} \\
&= \int_{ - \infty }^\infty  {g(y - {x_1})} f({x_1})d{x_1}
\end{split}
\end{equation}

There are a few potential problems with equation 13. It is possible the integral does not have a closed form solution or the integral may be so intractable that an analytical solution is not practical. The good news is that we can use Monte Carlo simulation techniques to over come these problems.

A closer examination of the integral in equation 13 reveals that the form is nothing other than the formal definition of the expected value. If we apply the classical Monte Carlo integration techniques then we arrive at the following general equations:

$${E_f}[h(X)] = \int\limits_\chi  {h(x)f(x)dx} $$

where, $\chi$ is the set where the random variable $X$ takes its value, then $\chi$ is equal to the support of the density $f$.

After we integrate out $f(x)$ we are left with the following equation which can be directly converted into a Monte Carlo integration algorithm:

\begin{equation}
\tag{General}
\overline {{h_n}}  = {1 \over n}\sum\limits_{i = 1}^n {h({x_i})}
\end{equation}

\begin{equation}
{f_Y}(y) = {1 \over n}\sum\limits_{i = 1}^n {f(y - {x_2})} d{x_2}
\end{equation}

The below plots show the results from the convolution method. The initial values for the distribution parameters are the same values listed in Table 1. We can see in the second plot that the true density (red) and the Monte Carlo simulation of the convolution (green) are all but identical. This indicates that the Monte Carlo method is highly successful at uncovering the true density.

\bigskip

```{r, echo=F, fig.height=3}

# Problem 2.c Plots -----------------------------------------------------------

par(mfrow= c(1, 2), ps= 10, cex= 0.55, cex.main= 1.5)

hist(Y, prob= TRUE, xlab= "Y", ylab= "p(Y)", xlim=c(-30, 30))

lines(Y.grid, true.density, col= "red", lty= 2, lwd= 2)

legend(4, 0.1,  c("True Density of Y"), lty= c(2), col= c("red"))

hist(Y, prob= TRUE, xlab= "Y", ylab= "p(Y)", xlim= c(-30, 30),
     main= "Convolution Method - Histogram of Y")

lines(Y.grid, true.density, col = "red", lty= 2, lwd= 4)

lines(Y.grid, convolution.Y, col = "lightgreen", lty= 1)

legend(4, 0.1,  c("True Density of Y", "Convolution of Y"), 
       lty=c(2, 1), col=c("red", "lightgreen"))

```


### (d) Repeat the two approaches described in (b) and (c) above for ${X_1} \sim Gamma({\alpha _1},{\beta _1})$ and ${X_2} \sim Gamma({\alpha _2},{\beta _2})$. Note, unless $\beta_1$ = $\beta_2$, the resulting distribution of $Y$ is not friendly. Also be cautious of the support.



\newpage

# Problem 3

Game of chance, the game of craps and a betting strategy are going to be explored in this Monte Carlo simulation experiment.

### (a) Do a search and outline the rules of craps.




### (b) Your initial bet will be 10 dollars. The return on any bet will be the amount that you bet; e.g., if you bet 10 dollars and win, then you win 10 dollars. You are going to play exactly 10 games of craps. If you lose on the previous game, your next bet requires you to “double down,” that is, if on one game you bet X dollars and lose, on the next game you have to bet 2X dollars. If you win on the previous game, your next bet will be 10 dollars.




### (c) Write a Monte Carol simulation to find your expected winnings after the 10 games. Also examine the distribution of your expected winnings. Does it appear that this is a good betting strategy?


\newpage
\singlespace

---
nocite: | 
  @2017arXiv170501064L, @Kerman:2011un, @Gelman:2014dr, @Grinstead:1997ww, 
  @casella2002statistical, @Robert:2010vu
...

# References and Appendix

## References:


