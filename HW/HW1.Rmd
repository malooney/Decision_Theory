---
title: "STAT 5370 â€“ Decision Theory"
subtitle: "Homework 1"
author: "Matthew Aaron Looney"
date: "6/15/2018"
output: 
  pdf_document: 
    fig_caption: yes
    toc: yes
    toc_depth: 4
header-includes:
- \usepackage{graphicx}
- \usepackage{rotating}
- \usepackage{longtable}
- \usepackage{amssymb,amsmath}
- \usepackage{dcolumn}
- \usepackage{setspace}
- \usepackage{mathrsfs}
- \usepackage{eso-pic,graphicx,transparent}
fontsize: 12pt
geometry: margin = 1.25in
---

```{r, echo=F, warning=F, message=F, cache=F}

# Housekeeping ----------------------------------------------------------------
rm(list=ls())
cat("\014")
#library(datasets)
#data(faithful)
library(feather)
library(dplyr)
library(stargazer)

```

```{r, echo=F, warning=F, message=F, cache=T}

# Load Data -------------------------------------------------------------------
LA_data_2010 <- read_feather("/Users/malooney/Google Drive/digitalLibrary/*MS_Thesis/MS_Thesis/data/LA_data_2010_feather")

# Add volume measures ---------------------------------------------------------
oz <- round(data.frame(oz=LA_data_2010$VOL_EQ.x* 288))
total_oz <- (oz* LA_data_2010$UNITS); 
colnames(total_oz) <- "total_oz"
total_gal <- (0.0078125* total_oz); 
colnames(total_gal) <- "total_gal"
dollarPerGal <- LA_data_2010$DOLLARS/ total_gal; 
colnames(dollarPerGal) <- "dollarPerGal"

LA_data_2010_manip <- cbind(LA_data_2010, oz, total_oz, total_gal, 
                            dollarPerGal)

rm(oz, total_gal, total_oz, dollarPerGal, LA_data_2010)

# Remove zero data ------------------------------------------------------------
LA_data_2010_manip <- filter(LA_data_2010_manip, L5 !="ALL BRAND")
LA_data_2010_manip <- filter(LA_data_2010_manip, dollarPerGal !="Inf")

# Explore Brands, Firms, and percenatges of Brands by Firms -------------------
#uniqueBrands <- data.frame(table(LA_data_2010_manip$L5))
#uniqueBrands <- arrange(uniqueBrands, desc(Freq))
#(sum(uniqueBrands[1:20,2]) / nrow(LA_data_2010_manip)) * 100

#uniqueBrands_all <- data.frame(Brand = rep(LA_data_2010_manip$L5, 
#                                       LA_data_2010_manip$UNITS), 
#                           y = sequence(LA_data_2010_manip$UNITS))

#uniqueBrands <- data.frame(table(uniqueBrands_all$Brand))
#uniqueBrands <- arrange(uniqueBrands, desc(Freq))
#prcntBrandRep <- (sum(uniqueBrands[1:60,2]) / nrow(uniqueBrands_all)) * 100

#uniqueChains <- data.frame(table(LA_data_2010_manip$MskdName))
#uniqueChains <- arrange(uniqueChains, desc(Freq))
#uniqueFirms <- data.frame(table(LA_data_2010_manip$L4))
#uniqueFirms <- arrange(uniqueFirms, desc(Freq))
#uniqueConglomerates <- data.frame(table(LA_data_2010_manip$L3))
#uniqueConglomerates <- arrange(uniqueConglomerates, desc(Freq))

#rm(uniqueBrands_all)

uniqueWeeks <- unique(LA_data_2010_manip$WEEK)

LA_data_2010_manip1 <- filter(LA_data_2010_manip, MskdName=="Chain79")
LA_data_2010_manip2 <- filter(LA_data_2010_manip1, L5=="ANCHOR STEAM BEER")
LA_data_2010_manip3 <- filter(LA_data_2010_manip2, upc=="00-01-72783-00100")


mainData <- data.frame(week=c(1:52), units = c(1:52), gallons = c(1:52), 
                       `Calendar week starting on` = c(1:52), 
                       weekNum = uniqueWeeks)

for(i in 1:length(uniqueWeeks)){
  temp <- filter(LA_data_2010_manip3, WEEK== uniqueWeeks[i])
  temp1 <- sum(temp$UNITS)
  temp2 <- sum(temp$total_gal)
  mainData[i,2] <- temp1
  mainData[i,3] <- temp2
  mainData[i,4] <- as.character(unique(LA_data_2010_manip$`Calendar week starting on`)[i])
  
}

rm(i, temp, temp1, temp2, uniqueWeeks, LA_data_2010_manip, 
   LA_data_2010_manip1, LA_data_2010_manip2, LA_data_2010_manip3)

```

\newpage

\doublespacing

# Problem 1

In this problem we will consider developing a Bayesian model for Poisson
iid data; i.e., our observed data will consist of $Y_1,...,Y_n \mathop  \sim \limits^{iid} Poisson(\lambda )$. Recall, a random variable $Y$ is said to follow a Poisson distribution, with mean parameter $\lambda$, if its pmf is given by

\begin{equation}
p(y|\lambda ) = \frac{{{e^{ - \lambda }}{\lambda ^y}}}{{y!}}I(y \in \{ 0,1,2,...\} )
\end{equation}

Note, the Poisson model is often used to analyze count data.

### (a)  For the Poisson model, identify the conjugate prior. This should be a general class of priors.

If we assume the Gamma distribution as a prior for the Poisson model: $p(\lambda)=\frac{{{\beta ^\alpha }}}{{\Gamma (\alpha )}}{\lambda^{\alpha  - 1}}{e^{ - \beta \lambda}}  \propto {\lambda^{\alpha  - 1}}{e^{ - \beta \lambda}}$; where $\alpha>0$ (shape parameter) and $\beta>0$ (rate parameter) then this choice of prior is also the conjugate prior for the Poisson model.


### (b)  Under the conjugate prior, derive the posterior distribution of $\lambda|y$. This should be a general expression based on the choice of the hyper-parameters specified in your prior.

To derive the posterior distribution we need the likelihood function associated with the Poisson model:

\begin{equation}
L(\lambda |y) = \prod\limits_{i = 1}^n {\frac{{{e^{ - \lambda }}{\lambda ^y}}}{{y!}}}  = \frac{{{e^{ - n\lambda }}{\lambda ^{\sum\limits_{i = 1}^n {{y_i}} }}}}{{\prod\limits_{i = 1}^n {y!} }}
\end{equation}

In general, the posterior distribution is defined as:

\begin{equation}
p(\lambda |y) = \frac{{p(y|\lambda )p(\lambda )}}{{\int\limits_A {p(y|\lambda )p(\lambda )d\lambda } }} \propto \underbrace {p(y|\lambda )}_{{\text{L(}}\lambda {\text{)}}}\underbrace {p(\lambda )}_{{\text{Prior}}}
\end{equation}

With a Poisson DGP and a Gamma prior the derived posterior distribution is defined by:

\begin{equation}
\begin{aligned}
  p(\lambda | y) &= {e^{ - n\lambda }}{\lambda ^{\sum\limits_{i = 1}^n {{y_i}} }} \cdot {\lambda ^{\alpha  - 1}}{e^{ - \beta \lambda }} \\ 
                 &= {\lambda ^{\sum\limits_{}^{} {{y_i} + \alpha  - 1} }} \cdot {e^{ - (n - \beta )\lambda }} \\ 
\end{aligned}
\end{equation}

The posterior distribution of $\lambda$ under the Gamma prior is $Gamma( {\sum {{y_i}}  + \alpha }, {n + \beta })$, where $\alpha$ and $\beta$ are the hyperparamaters of the Gamma distribution.


### (c)  Find the posterior mean and variance of $\lambda|y$. These should be general expressions based on the choice of the hyper-parameters specified in your prior.

The mean and variance of the Gamma distribution is given by the following expressions: $E(\lambda)=\frac{\alpha}{\beta}$; $Var(\lambda)=\frac{\alpha}{\beta^2}$.

Then the posterior mean and variance  of $\lambda|y$ is:

\begin{equation}
\begin{aligned}
E(\lambda |y) &= \frac{\alpha }{\beta } = \frac{{\sum {{y_i}}  + \alpha }}{{n + \beta }} \\
Var(\lambda |y) &= \frac{\alpha }{{{\beta ^2}}} = \frac{{\sum {{y_i}}  + \alpha }}{{{{(n + \beta )}^2}}}
\end{aligned}
\end{equation}


### (d)   Obtain the MLE of $\lambda$. Develop and discuss a relationship that exists between the MLE and posterior mean identified in (c).

Starting from the Poisson distribution, $p(y|\lambda ) = \frac{{{e^{ - \lambda }}{\lambda ^y}}}{{y!}}$, we obtain the likelihood function:

\begin{equation}
L(\lambda |{y_1},{y_2},...,{y_n}) = \prod\limits_{i = 1}^n {\frac{{{e^{ - \lambda }}{\lambda ^{{y_i}}}}}{{{y_i}!}}} 
\end{equation}

Then, taking log of equation 6 and differentiating w.r.t $\lambda$ we can obtain an expression for $\hat\lambda$:

\begin{equation}
\begin{gathered}
  \log L(\lambda |{x_1},{x_2},...,{x_n}) =  - n\lambda  - \sum\limits_{i = 1}^n {\log ({y_i}!) + \log (\lambda )\sum\limits_{i = 1}^n {{y_i}} }  \hfill \\
  \frac{{\partial \log L( \cdot )}}{{\partial \lambda }} =  - n + \frac{{\sum\limits_{i = 1}^n {{y_i}} }}{\hat\lambda } \mathop  = \limits^{set}  0 \hfill \\
  \hat \lambda  = \frac{{\sum\limits_{i = 1}^n {{y_i}} }}{n} \hfill \\ 
\end{gathered} 
\end{equation}


We can now see that when the hyperparamaters of the posterior mean (part c, equation 5) are zero the posterior mean collapses to the MLE estimator of $\lambda$. This shows that the MLE estimator of $\lambda$ and the posterior mean of $\lambda$ are related by a weighted average. Further, we can see that the MLE estimator for $\lambda$ is $\sum\nolimits_{i = 1}^n {{y_i}} /n$ and the posterior mean are a combination of the weighted average of $\sum\nolimits_{i = 1}^n {{y_i}} /n$ and the prior mean.


### (e)  Write two separate R programs which can be used to find both a (1-$\alpha$)100% equal-tailed credible interval and a (1-$\alpha$)100% HPD credible interval for the Poisson model. These programs should take as arguments the following inputs: the observed data, prior hyper-parameters, and significance level.

```{r, echo=F}

# Equal Tail Credible Interval ------------------------------------------------
equalTail_CI <- function(y, a0, b0, alpha=0.05, displayPlot=1){
  
  n= length(y)
  a_posterior= a0+ n* mean(y)
  b_posterior= b0+ n
  critI <- qgamma(p=c(0.025, 0.975), shape= a0+ n* mean(y), rate= b0+ n)
  
    if(displayPlot==1){
    th = seq(0, 10, length=1000)
    
    plot(th, dgamma(th, a_posterior, b_posterior),
         t="l", lty=1,xlab=expression(lambda),
         ylab="Posterior Density", 
         col="blue")
    #abline(h= h*dmode)
    segments(critI[2], 0, critI[2], dgamma(critI[2], a_posterior, b_posterior),
             col="red", lty=2)
    segments(critI[1], 0, critI[1], dgamma(critI[1], a_posterior, b_posterior),
             col="red", lty=2)
    
    title(main="Plot of Equal Tail Credible Interval")
    mtext(bquote(paste("P(", .(round(critI[1], 4))," < ", Lambda, " < ",
                       .(round(critI[2], 4)), " | " , y, ") = ",
                       .(round(0.95, 4)))
                       ), 3, line=0)
    } else {}
  
  return(critI)
  
}

# HDP Credible Interval -------------------------------------------------------

HPD.h = function(y, h=.1, a_posterior=a1, b_posterior=b1, displayPlot=0, ...){

  if(a_posterior >= 1){
    mode = (a_posterior - 1) / (b_posterior)
    dmode = dgamma(mode, a_posterior, b_posterior)
    } else {
      print("Can not calculate mode. 
            The b_posterior must be greater than or equal to zero!") 
    }
  
  lt = uniroot(f=function(x){
    dgamma(x, a_posterior, b_posterior) / dmode - h}, 
    lower=0, 
    upper=mode)$root
  
  ut = uniroot(f=function(x){ 
    dgamma(x,a_posterior, b_posterior) / dmode - h}, 
    lower=mode, 
    upper=100)$root
  
  coverage = pgamma(ut, a_posterior, b_posterior) - 
    pgamma(lt, a_posterior, b_posterior)
  
  if(displayPlot==1){
    th = seq(0, 10, length=1000)
    
    plot(th, dgamma(th, a_posterior, b_posterior),
         t="l", lty=1,xlab=expression(lambda),
         ylab="Posterior Density", 
         col="blue")
    #abline(h= h*dmode)
    segments(ut, 0, ut, dgamma(ut, a_posterior, b_posterior), col="red")
    segments(lt, 0, lt, dgamma(lt, a_posterior, b_posterior), col="red")
    
    title(main="Plot of HPD Credible Interval")
    mtext(bquote(paste("P(", .(round(lt, 4))," < ", Lambda, " < ",
                       .(round(ut, 4)), " | " , y, ") = ",
                       .(round(coverage, 4)))
                       ), 3, line=0)
  } else {}
  
  return(c(lt, ut, coverage, h))
  
}

HPD = function(h, y, alpha){
  cov = HPD.h(y, h = h)[3]
  res = (cov - (1 - alpha))^2
  return(res)
}


```

Please see Appendix A


### (f)  Find a data set which could be appropriately analyzed using the Poisson model. This data set should be of interest to you, and you should discuss, briefly, why the aforementioned model is appropriate; e.g., consider independence, identically distributed, etc. etc. You will also need to provide the source of the data.

```{r echo=F, fig.height=3.5, fig.cap="\\label{fig:figs}"}

par(mfrow=c(1,1), ps = 10, cex = 0.75, cex.main = 1)

plot(x = mainData[,1], y = mainData[,2], type="l", 
     main="Unit of Beer Sold per Week", 
     xlab="Week", 
     ylab="Units")

#hist(mainData[,2], main="Histogram of Unit of Beer Sold per Week")

```

In this study I will use a Poisson model to analyze market level data for beer. A market is defined in the follow way: units of beer sold per week during 2010 in Los Angeles, California over a 52 week period at the chain/brand level. The market level sales data are obtained from Information Resource Inc. (IRI) and consist of all dollar by volume sales for a brand of beer at a specific chain during a one week period. In this data set there are seven chains represented in Los Angeles; I have chosen a single chain for this analysis. The chain name is masked for privacy. The brand is a California craft brew named Anchor Steam Beer. The brewery is located in San Francisco, California and has a total annual production output of 132,000 barrels.  
  
Given the count nature of this data, using a Poisson model is appropriate. Assuming independent and identically distributed data is reasonable since there is no expectation that aggregate beer sales from one week to the next should be related in any meaningful way. Of course this assumption is not very hard to counter with some thought but I will assume iid data in this analysis.
  
A single unit of beer consists of 6 - 12 ounce bottles of Anchor Steam Beer.  
  
See Appendix B for data. Raw data available upon request.

\bigskip
\bigskip
\bigskip

```{r echo=F, fig.height=3.5, fig.cap="\\label{fig:figs}"}

par(mfrow=c(1,1), ps = 10, cex = 0.75, cex.main = 1)

# plot(x = mainData[,1], y = mainData[,2], type="l", 
#      main="Unit of Beer Sold per Week", 
#      xlab="Week", 
#      ylab="Units")

hist(mainData[,2], main="Histogram of Unit of Beer Sold per Week", 
     xlab="Week")

```

### (g)  Analyze the data set you have selected in (e). Provide posterior point estimates of $\lambda$, credible intervals, etc. etc. Your analysis should be accompanied by an appropriate discussion of your findings.

```{r, echo=F, warning=F, message=F}

# Beer data -------------------------------------------------------------------
sum_y = sum(mainData$units)
n = length(mainData$units)
mu_y.Freq <- mean(mainData$units)
var_y.Freq <- var(mainData$units)

# prior and posterior ---------------------------------------------------------
mu_0 <- 4
sigma2_0 <-8

b0 <- mu_0 / sigma2_0
a0 <- mu_0 * b0

# posterior parameters --------------------------------------------------------
a1= a0 + sum_y
b1 = b0 + n

posterior.mean = a1/b1
posterior.var = a1/b1^2

h.final= optimize(HPD, c(0, 1), y= y, alpha = 0.05)$minimum
hpdOutput <- HPD.h(y= mainData$units, h= h.final, displayPlot= 0)

```

The conjugate prior for a Poisson model is the Gamma distribution (equation 4). Using the conjugate prior and the likelihood function we have derived the posterior mean and variance (equation 5). We can use these previously derived equations to obtain the insight into our data set on beer sales.  
  
The hyperparameters for the Gamma prior were set at $\alpha_0=2$, $\beta_0=0.5$. These parameters were chosen based on the density plot of the data and the overlay of the shape of the Gamma distribution using the choice of hyperparamaters (see figure 3). The choice of hyperparamater had little effect on the posterior mean and variance.

Then the posterior mean and variance  of $\lambda|y$ is:

\begin{equation}
\begin{aligned}
E(\lambda |y) &= \frac{\alpha }{\beta } = \frac{{\sum {{y_i}}  + \alpha }}{{n + \beta }} =`r posterior.mean ` \\
Var(\lambda |y) &= \frac{\alpha }{{{\beta ^2}}} = \frac{{\sum {{y_i}}  + \alpha }}{{{{(n + \beta )}^2}}} = `r posterior.var `
\end{aligned}
\end{equation}

Given the data the calculated posterior mean is a very reasonable estimate.  
  
We can calculate the Equal Tail Credible Intervals and the HPD Credible Intervals to determine the 95% probability that $\lambda$ falls within some upper and lower bound.  
  
The Equal Tail Credible Intervals is [`r equalTail_CI(y=mainData$units, a0=a0, b0=b0, displayPlot=0) `] (see plot - Figure 4).

The HPD Credible Intervals is [`r HPD.h(y= mainData$units, h= h.final, displayPlot= 0) `] (see plot - Figure 4).

By evaluating the credible Intervals we notice that they are very close. We can say that there is a 95% probability that the true value is $\lambda$ is between [`r HPD.h(y= mainData$units, h= h.final, displayPlot= 0) `]. Further, we can conclude that on average we can expect to see `r posterior.mean ` units of beer sold per week during 2010 in Los Angeles, California over a 52 week period at the chain/brand level.  
  
The mean estimate is limited since it is only based of past data and does not account for future uncertainty. A forecasting study would be needed to gain more insight into how future sales might look in this market.



```{r, echo=F, fig.height=7, fig.cap="\\label{fig:figs}"}

par(mfrow=c(2,2), ps = 10, cex = 0.75, cex.main = 1)

# Plot paramaters -------------------------------------------------------------

grid = seq(0.1, 20, length=10000)

grid.priorpmf = dgamma(grid, shape= a0, rate= b0)

priorpmf.grid = dgamma(grid, shape= a0, rate= b0)

# Plot Prior ------------------------------------------------------------------
plot(grid, priorpmf.grid,
     col='red', type="l", 
     main="Prior Density", xlab=expression(lambda), ylab="Prior Density")

lines(density(mainData$units), lty=2)

legend(12, .18,  c("Prior", "Data"), lty=c(1,2), col=c("red", "black"))

# Plot Likelihood -------------------------------------------------------------
likelihood.grid = dpois(sum_y, lambda = n * grid, log=F)

plot(x = grid, y = likelihood.grid,
     main="Likelihood of Lambda", 
     col='green',type="l", 
     xlab=  expression(lambda), 
     ylab= expression(L(lambda)))

# Plot Posterior --------------------------------------------------------------
posteriorpmf.grid=dgamma(grid,shape= a1, rate= b1)
plot(grid, posteriorpmf.grid,
     main="Posterior Density",  ylab="Posterior Density", col='blue', type="l",
     xlab=expression(lambda))

# Plot densities together -----------------------------------------------------
plot(grid, posteriorpmf.grid, ylab="density",
     main="Densities \n Gamma Prior/Posterior (red/blue)\n",
     type="n", xlab=expression(lambda))

lines(grid, priorpmf.grid, col='red') # prior

lines(grid, posteriorpmf.grid, col='blue') # posterior

lines(density(mainData$units), lty=2) # data

lines(grid, 20*likelihood.grid, col="green") # liklihood

legend(10, 1.35,  c("Prior", "Data", "Likelihood", "Posterior"), 
       lty=c(1,2,1, 1), col=c("red", "black", "green", "blue"))

```

```{r, echo=F, fig.height=4, fig.cap="\\label{fig:figs}"}

par(mfrow=c(1,2), ps= 10, cex= 1, cex.main= 1)

eTailOutput <- equalTail_CI(y=mainData$units, a0=a0, b0=b0, displayPlot=1)

h.final= optimize(HPD, c(0, 1), y= y, alpha = 0.05)$minimum
hpdOutput <- HPD.h(y= mainData$units, h= h.final, displayPlot= 1)

```



\newpage

# Appendix A - Code for Problem 1, part e.

\begin{verbatim}


# Equal Tail Credible Interval ------------------------------------------------
equalTail_CI <- function(y, a0, b0, alpha=0.05, displayPlot=1){
  
  n= length(y)
  a_posterior= a0+ n* mean(y)
  b_posterior= b0+ n
  critI <- qgamma(p=c(0.025, 0.975), shape= a0+ n* mean(y), rate= b0+ n)
  
    if(displayPlot==1){
    th = seq(0, 10, length=1000)
    
    plot(th, dgamma(th, a_posterior, b_posterior),
         t="l", lty=1,xlab=expression(lambda),
         ylab="Posterior Density", 
         col="blue")
    #abline(h= h*dmode)
    segments(critI[2], 0, critI[2], dgamma(critI[2], a_posterior, b_posterior),
             col="red", lty=2)
    segments(critI[1], 0, critI[1], dgamma(critI[1], a_posterior, b_posterior),
             col="red", lty=2)
    
    title(main="Plot of Equal Tail Credible Interval")
    mtext(bquote(paste("P(", .(round(critI[1], 4))," < ", Lambda, " < ",
                       .(round(critI[2], 4)), " | " , y, ") = ",
                       .(round(0.95, 4)))
                       ), 3, line=0)
    } else {}
  
  return(critI)
  
}

# HDP Credible Interval -------------------------------------------------------

HPD.h = function(y, h=.1, a_posterior=a1, b_posterior=b1, displayPlot=0, ...){

  if(a_posterior >= 1){
    mode = (a_posterior - 1) / (b_posterior)
    dmode = dgamma(mode, a_posterior, b_posterior)
    } else {
      print("Can not calculate mode. 
            The b_posterior must be greater than or equal to zero!") 
    }
  
  lt = uniroot(f=function(x){
    dgamma(x, a_posterior, b_posterior) / dmode - h}, 
    lower=0, 
    upper=mode)$root
  
  ut = uniroot(f=function(x){ 
    dgamma(x,a_posterior, b_posterior) / dmode - h}, 
    lower=mode, 
    upper=100)$root
  
  coverage = pgamma(ut, a_posterior, b_posterior) - 
    pgamma(lt, a_posterior, b_posterior)
  
  if(displayPlot==1){
    th = seq(0, 10, length=1000)
    
    plot(th, dgamma(th, a_posterior, b_posterior),
         t="l", lty=1,xlab=expression(lambda),
         ylab="Posterior Density", 
         col="blue")
    #abline(h= h*dmode)
    segments(ut, 0, ut, dgamma(ut, a_posterior, b_posterior), col="red")
    segments(lt, 0, lt, dgamma(lt, a_posterior, b_posterior), col="red")
    
    title(main="Plot of HPD Credible Interval")
    mtext(bquote(paste("P(", .(round(lt, 4))," < ", Lambda, " < ",
                       .(round(ut, 4)), " | " , y, ") = ",
                       .(round(coverage, 4)))
                       ), 3, line=0)
  } else {}
  
  return(c(lt, ut, coverage, h))
  
}

HPD = function(h, y, alpha){
  cov = HPD.h(y, h = h)[3]
  res = (cov - (1 - alpha))^2
  return(res)
}


\end{verbatim}

\newpage

# Appendix B - Data

```{r, echo=FALSE, message=FALSE, results='asis', fig.height=3, results='asis'}

stargazer(mainData[,c(1,2,4)], header=F, type="latex", summary= FALSE, 
          rownames= FALSE, font.size = "scriptsize", 
          title="Data", 
          notes = "Units of Anchor Steam Beer sold in Los Angeles, CA (2010)")

```

